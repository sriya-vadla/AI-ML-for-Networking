# AIâ€‘MLâ€‘forâ€‘Networking

> **Intelligent traffic classification & anomaly detection for modern, encrypted networks.**
>
> â€¢ **Randomâ€¯Forest** flow classifierâ€ƒâ€¢ **Isolationâ€¯Forest** zeroâ€‘day detectorâ€ƒâ€¢ **Privacyâ€‘aware feature pipeline**

---

## Tableâ€¯ofâ€¯Contents

1. [Overview](#overview)
2. [Keyâ€¯Features](#key-features)
3. [Installation](#installation)
4. [Quickâ€¯Start](#quick-start)
5. [Projectâ€¯Structure](#project-structure)
6. [Usage](#usage)
7. [Sampleâ€¯Results](#sample-results)
8. [Roadmap](#roadmap)
9. [Contributing](#contributing)
10. [License](#license)

---

## Overview

`AIâ€‘MLâ€‘forâ€‘Networking` is a Python toolkit that tackles the blindâ€‘spot problem created by ubiquitous TLS, QUIC, and bursty IoT traffic.\
It combines classical ensemble learners with lightweight preprocessing to:

- **Classify** network flows as *benign* or *malicious* (multiâ€‘class labels supported).
- **Detect** neverâ€‘beforeâ€‘seen (zeroâ€‘day) anomalies in real time.
- **Preserve privacy** by aggregating sensitive packet fields onâ€‘prem while exporting only derived features.

Benchmarked on the NSLâ€‘KDD dataset, the reference pipeline achieves **Macroâ€‘F1 â‰ˆâ€¯0.92** and processes \~10â€¯k flowsâ€¯/â€¯sec on a laptopâ€‘class CPU.

---

## Keyâ€¯Features

- ðŸ” **Supervised Flow Classification** â€” Gridâ€‘searchâ€‘tuned Randomâ€¯Forest (50â€‘100 trees, depth â‰¤â€¯10).
- ðŸš¨ **Unsupervised Anomaly Detection** â€” Isolationâ€¯Forest with contamination â‰¤â€¯1â€¯%.
- ðŸ—„ï¸ **Modular Codebase** â€” swap models or datasets via config flags; fully reproducible.
- âš¡ **Resource Friendly** â€” runs in â‰¤â€¯2â€¯GBâ€¯RAM; no GPU required.
- ðŸ”’ **Privacy Layer** â€” optional feature aggregator to keep raw packet headers onâ€‘prem.

---

## Installation

```bash
# 1. Clone the repo
$ git clone https://github.com/sriya-vadla/AI-ML-for-Networking.git
$ cd AI-ML-for-Networking

# 2. Create a virtual env (recommended)
$ python -m venv venv && source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Install dependencies
$ pip install -r requirements.txt
```

> **Dataset** â€” NSLâ€‘KDD ARFF files are autoâ€‘downloaded on first run.\
> To use your own PCAP/CSV flows, see [`docs/DATASET.md`](docs/DATASET.md).

---

## Quickâ€¯Start

Train the classifier and anomaly detector, then run predictions:

```bash
# Train & evaluate (creates models/*.joblib)
$ python run_analysis.py --mode train

# Predict on a CSV of flow features
$ python run_analysis.py --mode predict --input sample/flows.csv --output predictions.csv
```

---

## Projectâ€¯Structure

```text
AI-ML-for-Networking/
â”‚
â”œâ”€â”€ preprocessing.py       # Data cleaning & feature engineering
â”œâ”€â”€ models.py              # ML pipelines & hyperâ€‘parameter search
â”œâ”€â”€ run_analysis.py        # Orchestration CLI
â”‚
â”œâ”€â”€ sample/                # Demo configs & example flows
â”œâ”€â”€ tests/                 # Unit & integration tests (pytest)
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ docs/                  # Extended documentation & diagrams
    â””â”€â”€ DATASET.md         # How to use custom datasets
```

---

## Usage

Common CLI flags:

```bash
--mode {train,predict,benchmark}
--input  path/to/flows.csv      # required for predict/benchmark
--output path/to/out.csv        # default: predictions.csv
--model_dir models/             # where .joblib files live
--log_level {info,debug}
```

See `python run_analysis.py --help` for the full list.

### Live Capture (experimental)

```bash
$ python run_analysis.py --mode live --iface eth0
```

> Requires `scapy` or `pyshark`. Packets are aggregated into flow features before inference.

---

## Sampleâ€¯Results

```
               precision    recall  f1-score   support

        Benign     0.95      0.96      0.96     9711
       DoS_Hulk     0.93      0.91      0.92     2315
   Probe_PortScan   0.89      0.88      0.88     1337
          â€¦

    Macroâ€‘F1 â‰ˆ 0.92
```

A full HTML report with confusion matrices is generated under `reports/latest/` after each training run.

---


## Contributing

1. **Fork** the repo & create your feature branch (`git checkout -b feat/my-feature`).
2. **Commit** your changes (`git commit -m 'feat: add my feature'`).
3. **Push** to the branch (`git push origin feat/my-feature`).
4. **Open a Pull Request** explaining *why* and *how*.

Please run `pytest` before submitting.\
See [`CONTRIBUTING.md`](CONTRIBUTING.md) for coding style & CI details.

---

## License

Distributed under the **MIT License**. See [`LICENSE`](LICENSE) for more information.
